{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-01 15:11:00.751\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36msrc.utils\u001b[0m:\u001b[36mdevice\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mUsing cpu device.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple, Optional\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from loguru import logger\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from pettingzoo.atari import combat_tank_v2\n",
    "from pettingzoo.atari import space_war_v2\n",
    "from pettingzoo.mpe import simple_v3\n",
    "from itertools import count\n",
    "from src.utils import (\n",
    "    \n",
    "    save_episode_as_gif,\n",
    "    loss_fn_dqn\n",
    ")\n",
    "from src.agent import Agent\n",
    "from src.agent_dqn import Agent_dqn\n",
    "from src.policy import ValueFunctionQ\n",
    "from src.buffer import ReplayBuffer\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "from PIL import Image\n",
    "from IPython.display import Image as IPImage\n",
    "import io\n",
    "\n",
    "plt.ion()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.set_default_dtype(torch.float32)\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input channels: 3\n",
      "Action space: 18\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# instantiate the environment\n",
    "\n",
    "# env = space_war_v2.env(render_mode=\"rgb_array\")\n",
    "env = combat_tank_v2.env(render_mode=\"rgb_array\", has_maze=False)\n",
    "env.reset(seed=SEED)\n",
    "\n",
    "# get the state and action dimensions\n",
    "observation = env.last()[0]  # Get initial observation\n",
    "H, W, C = observation.shape # (height, width, channels)\n",
    "action_dim = env.action_space(\"first_0\").n\n",
    "\n",
    "# num_actions = environment.action_space.n\n",
    "# state_dimension = environment.observation_space.shape[0]\n",
    "print(f\"Input channels: {C}\\nAction space: {action_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "################################## Hyper-parameters Tuning ##################################\n",
    "state_dimension: int = 16\n",
    "num_actions: int = action_dim\n",
    "EPOCHS: int = 1000\n",
    "HIDDEN_DIMENSION: int = 16\n",
    "LEARNING_RATE: float = 3e-3\n",
    "DISCOUNT_FACTOR: float = .97\n",
    "EPISODES: int = 3_000\n",
    "gamma = DISCOUNT_FACTOR\n",
    "BATCH_SIZE: int = 8\n",
    "\n",
    "#############################################################################################\n",
    "\n",
    "player_one = Agent_dqn('first_0', state_dimension, num_actions, HIDDEN_DIMENSION, LEARNING_RATE, BATCH_SIZE, obs_dim=(C, H, W), gamma=DISCOUNT_FACTOR)\n",
    "player_two = Agent_dqn('second_0', state_dimension, num_actions, HIDDEN_DIMENSION, LEARNING_RATE, BATCH_SIZE, obs_dim=(C, H, W), gamma=DISCOUNT_FACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-12-01 15:11:03.401\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mEpoch:    1/2 \t| Winner: second_0   \t| Steps: 6001\u001b[0m\n",
      "\u001b[32m2024-12-01 15:11:03.425\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mPlayer: first_0 \t| Cache Size: 3001 \t| Loss: 1778721.80939 \t| Reward: 0.000\u001b[0m\n",
      "\u001b[32m2024-12-01 15:11:03.444\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mPlayer: second_0 \t| Cache Size: 3001 \t| Loss: 15962730.34811 \t| Reward: 0.000\u001b[0m\n",
      "\u001b[32m2024-12-01 15:11:03.666\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m25\u001b[0m - \u001b[1mEpoch:    2/2 \t| Winner: second_0   \t| Steps: 883\u001b[0m\n",
      "\u001b[32m2024-12-01 15:11:03.675\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mPlayer: first_0 \t| Cache Size: 442 \t| Loss: 6478208.30345 \t| Reward: 0.000\u001b[0m\n",
      "\u001b[32m2024-12-01 15:11:03.690\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1mPlayer: second_0 \t| Cache Size: 442 \t| Loss: 16879117.48886 \t| Reward: 0.000\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "all_players = [player_one, player_two]\n",
    "agents_map = dict(zip(env.agents, all_players))\n",
    "\n",
    "agent_scores =  {k: 0 for k in env.agents}\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # run one episode \n",
    "    env.reset()\n",
    "    for step, agent_name in enumerate(env.agent_iter()):\n",
    "        \n",
    "        agent = agents_map[agent_name]\n",
    "        try:\n",
    "            win = agent.take_action(env)\n",
    "        except:\n",
    "            # env.reset()\n",
    "            # winner = agent_name\n",
    "            break\n",
    "            #continue\n",
    "        \n",
    "        if win or step > 2*EPISODES:\n",
    "            agent_scores[agent_name] += int(win)\n",
    "            winner = agent_name\n",
    "            break\n",
    "    \n",
    "    logger.info(f'Epoch: {epoch+1:4}/{EPOCHS} \\t| Winner: {winner:10} \\t| Steps: {step}')\n",
    "    for player in all_players:\n",
    "        loss, reward = player.optimize(loss_fn_dqn)\n",
    "        logger.debug(f\"Player: {player.name} \\t| Cache Size: {len(player.cache)} \\t| Loss: {loss*1e8:.5f} \\t| Reward: {reward :.3f}\")\n",
    "        player.clear_cache()\n",
    "        player.save()\n",
    "    \n",
    "    \n",
    "    if (epoch+1) % 50 == 0:\n",
    "        save_dir = \"episodes\"\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        save_episode_as_gif(env, agents_map, save_path=f\"{save_dir}/epoch_{epoch+1}.gif\", fps=60)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_path = save_episode_as_gif(env, agents_map, fps=60)\n",
    "IPImage(open(gif_path,'rb').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
